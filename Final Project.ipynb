{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets: Project Intro\n",
    "### For this project, we will be using this data set of twitter post text, found on kaggle. The goal is to come up with an algorithm that most accuractely classifies a tweet as indicative of a real disaster or not a real disaster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/benjamin.mok/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benjamin.mok/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import *\n",
    "from sklearn import metrics\n",
    "\n",
    "# SK-learn Decomp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# NLP processors\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will read in the \"train\" dataset, which contains the correct labels. We will split the data 50/50 between train and dev (or test). For ease of analysis and text processing, the data will be further split into \"pos\" (label = 1) and \"neg\" (label = 0) dataframes and text only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some data metrics\n",
      "\n",
      "Shape of train data: (5438, 5)\n",
      "\n",
      "Missing data in each column:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "Number of disaster tweets:\n",
      "0    2996\n",
      "1    2442\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read in data\n",
    "# df = pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv')\n",
    "df = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "# sample the data, acts as shuffling the data on row\n",
    "\n",
    "#50/50 split between train and dev\n",
    "# allocate more for traiing if we do it this way, i'll run some\n",
    "# analysis to see if my cluster bootstrap can imrpove the models we run.\n",
    "numtest = int(len(df)/3.5)\n",
    "df_train = df[numtest:].reset_index(drop=True)\n",
    "df_test = df[:int(numtest/2)].reset_index(drop=True)\n",
    "df_dev = df[int(numtest/2):numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = df_train.text, df_train.target\n",
    "dev_data, dev_label = df_dev.text, df_dev.target\n",
    "test_data, test_label = df_test.text, df_test.target\n",
    "\n",
    "#split into disaster and non disaster data\n",
    "df_neg = df_train.loc[df_train.target == 0]\n",
    "df_pos = df_train.loc[df_train.target == 1]\n",
    "\n",
    "#split into disaster and nondisaster tweets only\n",
    "neg_text = df_neg.text\n",
    "pos_text = df_pos.text\n",
    "\n",
    "print(\"Some data metrics\\n\")\n",
    "print(\"Shape of train data:\", df_train.shape)\n",
    "print(\"\\nMissing data in each column:\\n\" + str(df.isnull().sum()))\n",
    "print(\"\\nNumber of disaster tweets:\\n\"+ str(train_label.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because the available tags, keyword and location, are sparse and method of construction are unclear to us, we wanted to create new tags for the text that we may be able to train later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id             keyword        location  \\\n",
      "0  8367                ruin         Belfast   \n",
      "1  4164               drown             NaN   \n",
      "2  9232   suicide%20bombing             NaN   \n",
      "3  7587            outbreak  Fukuoka, Japan   \n",
      "4  8816              sirens       Hollywood   \n",
      "5  9673             tornado         Midwest   \n",
      "6  2538           collision             NaN   \n",
      "7   798              battle             NaN   \n",
      "8  7108            military          Alaska   \n",
      "9  7228  natural%20disaster             NaN   \n",
      "\n",
      "                                                text  target  \\\n",
      "0  And then I go a ruin it all with something awf...       0   \n",
      "1        @GraysonDolan only if u let me drown you ??       0   \n",
      "2  meek mill should join isis since he loves suic...       0   \n",
      "3  Families to sue over Legionnaires: More than 4...       1   \n",
      "4  @TravelElixir Any idea what's going on? I hear...       1   \n",
      "5  (OK)  Severe Thunderstorm Warning issued Augus...       1   \n",
      "6  my favorite lady came to our volunteer meeting...       1   \n",
      "7  Dragon Ball Z: Battle Of Gods (2014) - Rotten ...       0   \n",
      "8  Mike Magner Discusses A Trust Betrayed: http:/...       0   \n",
      "9  @ConnorFranta #AskConnor if you were a natural...       0   \n",
      "\n",
      "                             hashtag          mentions links  retweet  \\\n",
      "0                          [minions]                []   [t]    False   \n",
      "1                                 []    [GraysonDolan]    []    False   \n",
      "2                                 []                []    []    False   \n",
      "3              [News, check, follow]                []   [t]    False   \n",
      "4                                 []    [TravelElixir]    []    False   \n",
      "5                             [okwx]                []   [t]    False   \n",
      "6                                 []                []   [t]    False   \n",
      "7                                 []  [RottenTomatoes]   [t]    False   \n",
      "8  [military, veterans, environment]         [YouTube]   [t]    False   \n",
      "9                        [AskConnor]    [ConnorFranta]    []    False   \n",
      "\n",
      "   mentions_ind  hashtag_ind  links_ind  retweet_ind  \n",
      "0             0            1          1            0  \n",
      "1             1            0          0            0  \n",
      "2             0            0          0            0  \n",
      "3             0            1          1            0  \n",
      "4             1            0          0            0  \n",
      "5             0            1          1            0  \n",
      "6             0            0          1            0  \n",
      "7             1            0          1            0  \n",
      "8             1            1          1            0  \n",
      "9             1            1          0            0  \n"
     ]
    }
   ],
   "source": [
    "df_train['hashtag'] = df_train['text'].apply(lambda s: re.findall(r'#(\\w+)', s))\n",
    "df_train['mentions'] = df_train['text'].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "df_train['links'] = df_train['text'].apply(lambda x: re.findall(r\"http:\\/\\/(\\w+)\", x))\n",
    "df_train['retweet'] = df_train['text'].apply(lambda x: \"rt\" in x.lower().split())\n",
    "\n",
    "df_train['mentions_ind'] = df_train.mentions.apply(lambda y: 0 if len(y)==0 else 1)\n",
    "df_train['hashtag_ind'] = df_train.hashtag.apply(lambda y: 0 if len(y)==0 else 1)\n",
    "df_train['links_ind'] = df_train.links.apply(lambda y: 0 if len(y)==0 else 1)\n",
    "df_train['retweet_ind'] = df_train.retweet.apply(lambda y: 0 if y == False else 1)\n",
    "\n",
    "print(df_train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text classification problems, text pre-processing is a crucial part to prepping our data for analysis. This can be found in our text_clean function. Some pre-processing considerations we have made include:\n",
    "* removing numbers, symbols, and punctuation\n",
    "* standardizing to lowercase text\n",
    "* remove stop words\n",
    "* word stemming\n",
    "* trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stop_words):\n",
    "    #remove line breaks\n",
    "    text = re.sub(r\"\\n\",\"\",text)\n",
    "\n",
    "    #convert to lowercase \n",
    "    text = text.lower()\n",
    "\n",
    "    #remove digits and currencies \n",
    "    text = re.sub(r\"\\d+\",\"\",text) \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)      \n",
    "\n",
    "    #remove dates \n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "\n",
    "    #remove non-ascii\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n",
    "\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "\n",
    "    #remove hyperlinks\n",
    "    #text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # remove trailing spaces\n",
    "    text = re.sub(r'[ \\t]+$','', text)\n",
    "    \n",
    "    # remove stop words, yea don't remove stop words.\n",
    "    # filtered_tokens = [word for word in word_tokenize(text) if not word in stop_words]\n",
    "    # text = (\"\").join(filtered_tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data and strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data-> split normally\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "df_ = df.sample(frac=1).reset_index()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_full = []\n",
    "for i in df_.text:\n",
    "    processed_full.append(preprocess(i,stop_words))\n",
    "df_.text = processed_full\n",
    "\n",
    "numtest = int(len(df_)/3.5)\n",
    "df_train = df_[numtest:].reset_index(drop=True)\n",
    "df_test = df_[:int(numtest/2)].reset_index(drop=True)\n",
    "df_dev = df_[int(numtest/2):numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by normal splitting\n",
      "Dev:\n",
      "F1 Score: 0.8155\n",
      "Accuracy: 0.8199\n",
      "Test:\n",
      "F1 Score: 0.7856\n",
      "Accuracy: 0.7930\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes example run, using non clustered data first.\n",
    "# I'll use TF-IDF in this to vectorize data.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_test = m_nb.predict(tt_data)\n",
    "print('Metrics by normal splitting')\n",
    "print('Dev:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(dev_label, pred, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(dev_label, pred)))\n",
    "print('Test:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by cluster-splitting\n",
      "F1 Score: 0.8201\n",
      "Accuracy: 0.8452\n"
     ]
    }
   ],
   "source": [
    "def cluster_boot(df, n_clusters=2):\n",
    "    # Must accept pre-processed data as DF\n",
    "    \n",
    "    \"\"\" Pre-Cluster data before splitting to enhance generalization\"\"\"\n",
    "    dat = df.text\n",
    "    tfidf = TfidfVectorizer()\n",
    "    t_data = tfidf.fit_transform(dat)\n",
    "    pca = \n",
    "    cluster = KMeans(n_clusters=n_clusters).fit(t_data) # really bad clustering for loo\n",
    "    df['assign'] = cluster.labels_\n",
    "    \n",
    "    if n_clusters == 2:\n",
    "        if len(df[df['assign']==1]) > len(df[df['assign']==0]):\n",
    "            s, l = 0, 1\n",
    "        else:\n",
    "            s, l = 1, 0\n",
    "        len_valid = int(len(df[df['assign']==s])/2)\n",
    "        df_test = df[df['assign']==s][:len_valid].reset_index(drop=True)\n",
    "        df_dev = df[df['assign']==s][len_valid:].reset_index(drop=True)\n",
    "        df_train = df[df['assign']==l].reset_index(drop=True)\n",
    "    else:\n",
    "        groups = [] # list of two tuples of clusters\n",
    "        centroids = cluster.cluster_centers_\n",
    "    \n",
    "    # Simple prelim:: Sparse matrix for Spectral Clustering\n",
    "    # if cluster is more than 2 then use the majority of clusters\n",
    "    # closest to each other as training set and the rest as dev/test\n",
    "    # notes: this method did not work, the spectral takes too long for\n",
    "    # this size of a sparse matrix.\n",
    "    #     elif typ == 'cv':\n",
    "    #         cv = CountVectorizer()\n",
    "    #         t_data = cv.fit_transform(dat)\n",
    "    #         cluster = SpectralClustering(n_clusters=2).fit(t_data)\n",
    "    return (df_train, df_dev, df_test)\n",
    "\n",
    "df_train,df_dev,df_test = cluster_boot(df_)\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_t = m_nb.predict(tt_data)\n",
    "print('Metrics by cluster-splitting')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(dev_label, pred, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(dev_label, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the data has been cleaned and text pre-processed, we can begin exploring different algorithms. The three machine learning algorithms we will focus on are:\n",
    "* Naive Bayes\n",
    "* Logistic Regression\n",
    "* SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
