{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/benon33/w207-final/blob/main/Final%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06RwxdISKMHx"
   },
   "source": [
    "# Executive Summary \n",
    "##### **Context**: As smartphones have become more accessible across the world, social media outlets, such as Twitter, have become important means of communicating during times of an emergency. As a result, more organizations are interested in tracking conversations in this space to respond more quickly.\n",
    "\n",
    "**Problem**: However, a user's intent in a tweet isn't always clear. Words, such as \"ablaze\", can be used in multiple contexts. It can be descriptive of a sunset or it could be indicative of something more serious, like a forest fire. \n",
    "\n",
    "##### **Approach**: For this project, we will use a data set of twitter posts, found on [Kaggle](https://www.kaggle.com/c/nlp-getting-started). The goal is to identify an algorithm that most accuractely classifies a tweet as indicative of a real disaster or not a real disaster. \n",
    "\n",
    "**Summary**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cF1EguFuKMH1",
    "outputId": "3b1f19ed-d7fd-4ebe-d42d-e9dfeecf0088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geonamescache in /opt/conda/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.6.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.0)\n",
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.7/site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud) (8.3.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "!pip install geonamescache\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "\n",
    "# General libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import *\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# SK-learn decomp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for feature extraction from text\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# NLP processors\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tnACrp6CKMH2"
   },
   "outputs": [],
   "source": [
    "# TODO: this package nltk.download(wordnet) is for lemmatize to work\n",
    "# The download did not work so we have to use the below code.. lol\n",
    "# Commented this out because it wasn't working for me on my GCP instance - maybe we can run it if an environment variable is set?\n",
    "# The download worked for me\n",
    "\n",
    "# Please run this code and SSL will pop up a window for you to choose a\n",
    "# Module to download, click 'all packages' and scroll down\n",
    "# and select 'wordnet'. You only need to download it once. \n",
    "# Not tested in Google Colab :3\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "# nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG0zhV0rKMH4"
   },
   "source": [
    "# Exploratory Data Analysis: \n",
    "\n",
    "* Dataset contains 7613 observations (i.e. rows) and 5 features (i.e.columns)\n",
    "* Dataset contains missing values and spaces, indicated by NaN and %20 values\n",
    "* There are 61 missing values under the keyword column\n",
    "* There are 2533 missing values in the location column\n",
    "* The training set shows an imbalance between classification types (i.e. tweet is/is not about a disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PJMOgOyRy8m",
    "outputId": "4857f268-73c7-44d8-efb8-34e395609749"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\lwu31\\\\OneDrive - JNJ\\\\Documents\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3612/3345623810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading in the training data from the competition CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of train data: {} {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing data in each column:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\lwu31\\\\OneDrive - JNJ\\\\Documents\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "# Reading in the training data from the competition CSV\n",
    "train_data =  pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv')\n",
    "\n",
    "print(\"Shape of train data: {} {}\\n\".format(train_data.shape[0], train_data.shape[1]))\n",
    "print(\"Missing data in each column:\\n\" + str(train_data.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "fjc9o_24SN12",
    "outputId": "b7e67597-fc99-4a55-a937-ce3ee7c24eb0"
   },
   "outputs": [],
   "source": [
    "#CREDIT TO SOURCES: \n",
    "#https://stackoverflow.com/questions/28931224/adding-value-labels-on-a-matplotlib-bar-chart \n",
    "\n",
    "# Explore distribution of dataset \n",
    "sns.countplot(x = train_data['target'], palette = 'rocket')\n",
    "ax = plt.gca()\n",
    "y_max = train_data['target'].value_counts().max() \n",
    "ax.set_ylim([0, round(y_max)])\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2., 1700, '%d' % int(p.get_height()), \n",
    "            fontsize=12, color='white', ha='center', va= 'bottom')\n",
    "plt.title(\"Observations by Classification Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UawMeq-mUehG"
   },
   "source": [
    "## Understanding Duplicate Data\n",
    "\n",
    "There are 110 tweets in this dataset. When we explored further, we saw that duplicate tweets were not always classified in the same way. As a result, we decided to drop duplicate tweets from the training set to improve our model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "PbuWUL9iTPG6",
    "outputId": "cb8f4160-b727-4994-850f-2e6d02eceed0"
   },
   "outputs": [],
   "source": [
    "duplct_cnt = len(train_data['text'])-len(train_data['text'].drop_duplicates())\n",
    "train_data['duplicate_tweet'] = np.where(train_data['text'].duplicated(keep=False), 1, 0)\n",
    "print('The most common duplicate tweets are:')\n",
    "train_data[train_data.duplicate_tweet == 1].pivot_table(index='text', values='target', aggfunc='count').sort_values(by='target', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "0LPbMY_HV0vo",
    "outputId": "a42ab131-6df7-421f-8605-5339ecf41c61"
   },
   "outputs": [],
   "source": [
    "dups = train_data[train_data.duplicate_tweet == 1]\n",
    "print('Duplicate tweets classification:')\n",
    "dups[['id', 'text', 'target']].sort_values(by='text', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSVvZ6iSXyYV"
   },
   "source": [
    "## Location Data Considerations:\n",
    "\n",
    "Much of the location data is not classified consistently--that is, it contains: \n",
    "*   different levels of granularity (countries, states, cities)\n",
    "*   different versions of the same entity (e.g. United States, USA) \n",
    "*   unidentifiable location (e.g. ) \n",
    "\n",
    "After attempting to clean the data, we were able to categorize a significant amount of tweets by country, but the vast majority of tweets were unable to be classified correctly. As a result, we determined that it would be best not to include location data in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "2m7SSrxqVFXg",
    "outputId": "e4899c93-2b0f-4b3d-b729-196ab9265a29"
   },
   "outputs": [],
   "source": [
    "train_data[['location', 'target']].dropna(subset=['location']).groupby('location').count().sort_values(by='target', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi3gezXbZjKd"
   },
   "outputs": [],
   "source": [
    "#CREDIT TO SOURCES\n",
    "#https://medium.com/swlh/extracting-location-data-from-twitter-54c837144038\n",
    "#https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb\n",
    "#https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame\n",
    "#https://stackoverflow.com/questions/34962104/how-can-i-use-the-apply-function-for-a-single-column\n",
    "\n",
    "#import libraries \n",
    "import geonamescache\n",
    "from geonamescache.mappers import country\n",
    "\n",
    "#discuss text_clean function with team -- need it for location EDA \n",
    "def text_clean(text):\n",
    "  #remove line breaks\n",
    "  text = re.sub(r\"\\n\",\"\",text)\n",
    "\n",
    "  #convert to lowercase \n",
    "  text = text.lower()\n",
    "\n",
    "  #remove digits and currencies \n",
    "  text = re.sub(r\"\\d+\",\"\",text) \n",
    "  text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)      \n",
    "\n",
    "  #remove dates \n",
    "  text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "  text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "  text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "\n",
    "  #remove non-ascii\n",
    "  text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n",
    "\n",
    "  #remove punctuation\n",
    "  text = re.sub(r'[^\\w\\s]','',text) \n",
    "\n",
    "  #remove hyperlinks\n",
    "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "  #replace extra whitespaces with a single one \n",
    "  #text = re.sub(re.sub(' +', ' ', text)\n",
    "  return text\n",
    "\n",
    "#break out location identifiers into separate lists\n",
    "#US-specific identifiers\n",
    "city_state_mapping =  pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv')\n",
    "US_map = dict(zip(city_state_mapping.city, city_state_mapping.state))\n",
    "US_cities = set(city_state_mapping.city)\n",
    "US_states = set(city_state_mapping.state)\n",
    "\n",
    "#import mapping function\n",
    "mapper = country(from_key='iso3', to_key='name')\n",
    "mapper2 = country(from_key='iso', to_key='name')\n",
    "\n",
    "#country-specific identifiers \n",
    "gc = geonamescache.GeonamesCache()  \n",
    "country_names = gc.get_countries_by_names()\n",
    "countries = list(country_names.keys())\n",
    "countries = [c.lower() for c in countries]\n",
    "iso3 = [val[\"iso3\"].lower() for key, val in country_names.items() if \"iso3\" in val]\n",
    "#iso = [val[\"iso\"].lower() for key, val in country_names.items() if \"iso\" in val] risks too many incorrect categorizations\n",
    "\n",
    "#define dataframe; drop null values \n",
    "loc_list = train_data[['location', 'target']].dropna(subset=['location'])\n",
    "loc_list['location'] = loc_list['location'].apply(lambda x: text_clean(x))\n",
    "\n",
    "#tokenize the location from each user-generated geo-location (word_tokenize) \n",
    "loc_list = loc_list.location.apply(nltk.word_tokenize)\n",
    "\n",
    "#initialize empty list \n",
    "country = []\n",
    "\n",
    "#categorize by country based on list matches\n",
    "for location in loc_list:\n",
    "  l = set(location)\n",
    "\n",
    "  if l.intersection(countries):\n",
    "    ctry = list(l.intersection(countries))\n",
    "    country.append(ctry[0])\n",
    "    continue\n",
    "\n",
    "  if l.intersection(US_states):\n",
    "    country.append('united states')\n",
    "    continue\n",
    "\n",
    "  if l.intersection(US_cities):\n",
    "    country.append('united states')\n",
    "    continue\n",
    "\n",
    "  if l.intersection(iso3):\n",
    "    ctry = list(l.intersection(iso3))\n",
    "    val = mapper(ctry[0].upper())\n",
    "    country.append(val.lower())\n",
    "    continue\n",
    "\n",
    "  # if l.intersection(iso):\n",
    "  #   ctry = list(l.intersection(iso))\n",
    "  #   val = mapper2(ctry[0].upper())\n",
    "  #   country.append(val.lower())\n",
    "  #   continue\n",
    "\n",
    "  else:\n",
    "    country.append('unknown')\n",
    "\n",
    "ldf = loc_list.to_frame()\n",
    "ldf['country'] = country\n",
    "ldf['target'] = train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "NMiP6Dq7Zz9m",
    "outputId": "85def71a-de12-4b17-bce8-6d38f3b0bf3d"
   },
   "outputs": [],
   "source": [
    "#plot locations by tweet type\n",
    "\n",
    "disaster_tweets = ldf.loc[ldf['target'] == 1]\n",
    "dt = disaster_tweets[['country', 'location']].groupby('country').count().sort_values(by='location', ascending=False).head(10)\n",
    "\n",
    "non_disaster_tweets = ldf.loc[ldf['target'] == 0]\n",
    "ndt = non_disaster_tweets[['country', 'location']].groupby('country').count().sort_values(by='location', ascending=False).head(10)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "fig.subplots_adjust(wspace=.5)\n",
    "#fig.suptitle('Tweets by Country')\n",
    "dt.plot.barh(ax=ax1, color='c', legend=None)\n",
    "ndt.plot.barh(ax=ax2, color='c', legend=None)\n",
    "\n",
    "ax1.set_title('Top 10 Countries (Disaster Tweets Only)')\n",
    "ax2.set_title('Top 10 Countries (Non-disaster Tweets Only)')\n",
    "ax1.xaxis.set_label_text('Count of Tweets')\n",
    "ax2.xaxis.set_label_text('Count of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faddqcSwa2Tk"
   },
   "source": [
    "## Top Keywords\n",
    "\n",
    "To better understand the keyword column in our data, we created word clouds that allowed us to understand which keywords were the most found in disaster vs non-disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsiGqXSIa2Tl",
    "outputId": "953915ac-a201-4a08-cd04-57787095e406"
   },
   "outputs": [],
   "source": [
    "# Code inspired by https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "# Gets us the frequency of words\n",
    "fdist_0 = FreqDist(train_data[train_data.target == 0 & train_data.keyword.notna()]['keyword'])\n",
    "fdist_1 = FreqDist(train_data[train_data.target == 1 & train_data.keyword.notna()]['keyword'])\n",
    "\n",
    "# Generates the WordCloud for disaster tweets\n",
    "wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(fdist_1)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Disaster Tweet Keywords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8dHYdooa2Tl",
    "outputId": "248732fd-8343-4af4-a15b-72b8f57e0e41"
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(fdist_0)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Non-Disaster Tweet Keywords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTyWAZpEa2Tm"
   },
   "source": [
    "We can see that there is a lot of conceptual overlap in the keywords in both the disaster and non-disaster tweets. For instance, note how \"body bags\" is considered a non-disaster tweet keyword, but seems like it should be a disaster keyword. It seems that all of the keywords could be considered as falling under the disaster realm. Additionally, we are not told how these keywords are generated for the Kaggle competition, which makes us not want to trust them for our modeling. Due to all of these factors, are leaning towards dropping them for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rJ7zxjibVQf"
   },
   "source": [
    "## Most Popular Bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Biqv0SVbUjg"
   },
   "outputs": [],
   "source": [
    "#CREDIT TO SOURCES: \n",
    "#https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "#https://stackoverflow.com/questions/43473736/most-common-2-grams-using-python\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "#create dataframe\n",
    "tweets = train_data[['text', 'target']]\n",
    "\n",
    "#segment disaster tweets\n",
    "dt = tweets.loc[tweets['target'] == 1]\n",
    "dt.columns = ['text', 'target']\n",
    "dt = dt['text']\n",
    "\n",
    "#segment non-disaster tweets\n",
    "ndt = tweets.loc[tweets['target'] == 0]\n",
    "ndt.columns = ['text', 'target']\n",
    "ndt = ndt['text']\n",
    "\n",
    "def count_top_bigrams(tweets):\n",
    "  #import stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "\n",
    "  #clean tweets\n",
    "  clean_tweets = [word_tokenize(text_clean(tweet)) for tweet in tweets]\n",
    "\n",
    "  #tokenize tweets and exclude stop words\n",
    "  token_list = []\n",
    "\n",
    "  for tweet in clean_tweets:\n",
    "    tokens = []\n",
    "    for token in tweet:\n",
    "      if token not in stop_words:\n",
    "        tokens.append(token)\n",
    "    token_list.append(tokens)\n",
    "\n",
    "  #group tokens as bigrams\n",
    "  bigram_list = []\n",
    "\n",
    "  for token in token_list: \n",
    "    bigram = list(ngrams(token, 2))\n",
    "    bigram_list.append(bigram)\n",
    "\n",
    "  #count bigrams\n",
    "  cnt = Counter()\n",
    "\n",
    "  for tweet in bigram_list:\n",
    "    for bigram in tweet:\n",
    "      cnt.update(nltk.bigrams(bigram))\n",
    "\n",
    "  top20 = cnt.most_common(20)\n",
    "  return top20\n",
    "\n",
    "#store output as dataframes\n",
    "db = count_top_bigrams(dt)\n",
    "disaster_bigrams = pd.DataFrame(db, columns=['bigram', 'count'])\n",
    "\n",
    "ndb = count_top_bigrams(ndt)\n",
    "non_disaster_bigrams = pd.DataFrame(ndb, columns=['bigram', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "OYK_YBAPcafG",
    "outputId": "f70459cc-c279-4a71-a4d2-7881fe153846"
   },
   "outputs": [],
   "source": [
    "#segment and plot by tweet type\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "fig.subplots_adjust(wspace=1)\n",
    "fig.suptitle('Top Bigrams by Tweet Type')\n",
    "sns.barplot(x = disaster_bigrams['count'], y = disaster_bigrams['bigram'], color='c', ax = axes[0])\n",
    "sns.barplot(x = non_disaster_bigrams['count'], y = non_disaster_bigrams['bigram'], color='c', ax=axes[1])\n",
    "\n",
    "axes[0].set_title('Top 20 Bigrams (Disaster Tweets Only)')\n",
    "axes[1].set_title('Top 20 Bigrams (Non-disaster Tweets Only)')\n",
    "axes[0].xaxis.set_label_text('Count of Tweets')\n",
    "axes[1].xaxis.set_label_text('Count of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6SXcw7Ma2Tm"
   },
   "source": [
    "## Adding Tweet Metadata as Features\n",
    "\n",
    "During our initial EDA, we noticed that each tweet had 4 pieces of metadata that we thought could be valuable for our models as additional features. These pieces of metadata were:\n",
    "\n",
    "1. Hashtags\n",
    "2. Mentions\n",
    "3. Retweet\n",
    "4. Links\n",
    "\n",
    "We decided to create 3 types of features per piece of metadata, with the exception of Retweet as Retweet can only be represented as a binary feature: \n",
    "\n",
    "1. A list of the words that comprise the metadata in the tweet, ex. for links, all of the links within a tweet\n",
    "2. A binary feature for whether a tweet has an instance of the metadata, ex. for links, whether a tweet has at least one link\n",
    "3. A numeric feature for the number of instances of metadata the tweet has, ex. for links, the number of links in a tweet\n",
    "\n",
    "We can create these features using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEvqrXNya2Tm",
    "outputId": "a09071a7-52a9-49e7-8d30-b17d63cbf909"
   },
   "outputs": [],
   "source": [
    "### First, let's create all of the regexes that we will need\n",
    "# The hashtag regex captures all word characters that come after a \"#\" symbol\n",
    "hashtag_regex = r'#(\\w+)'\n",
    "# The mention regex captures all word characters that come after an \"@\" symbol\n",
    "mention_regex = r'@(\\w+)'\n",
    "# In the data, all links are replaced with links that start with \"t.co\", for example http://t.co/lHYXEOHY6C\n",
    "# See https://help.twitter.com/en/using-twitter/url-shortener\n",
    "# The link regex captures all \"t.co\" links\n",
    "link_regex = r'(https?:\\/\\/t.co/\\w+)'\n",
    "# Retweets are indicated by the symbol \"rt\", because this is a simple string we don't need to use a heavyweight regex\n",
    "retweet_indicator = \"rt\"\n",
    "\n",
    "### Next, let's create feature 1 per relevant piece of metadata, i.e. a list of the words that comprise the metadata in the tweet\n",
    "def create_regex_finder_lambda(regex):\n",
    "    return lambda s: re.findall(regex, s)\n",
    "\n",
    "train_data['hashtags'] = train_data['text'].apply(create_regex_finder_lambda(hashtag_regex))\n",
    "train_data['mentions'] = train_data['text'].apply(create_regex_finder_lambda(mention_regex))\n",
    "train_data['links'] = train_data['text'].apply(create_regex_finder_lambda(link_regex))\n",
    "\n",
    "### Now let's create feature 2, a binary feature for whether a tweet has an instance of the metadata\n",
    "retweet_indicator_lambda = lambda s: 1 if retweet_indicator in s.lower().split() else 0\n",
    "generic_indicator_lambda = lambda s: 0 if len(s) == 0 else 1\n",
    "\n",
    "train_data['is_retweet'] = train_data['text'].apply(retweet_indicator_lambda)\n",
    "train_data['has_mentions'] = train_data.mentions.apply(generic_indicator_lambda)\n",
    "train_data['has_hashtags'] = train_data.hashtags.apply(generic_indicator_lambda)\n",
    "train_data['has_links'] = train_data.links.apply(generic_indicator_lambda)\n",
    "\n",
    "### Finally, let's create feature 3, a numeric feature for the number of instances of metadata the tweet has\n",
    "length_lambda = lambda s: len(s)\n",
    "\n",
    "train_data['mention_count'] = train_data.mentions.apply(length_lambda)\n",
    "train_data['hashtag_count'] = train_data.hashtags.apply(length_lambda)\n",
    "train_data['link_count'] = train_data.links.apply(length_lambda)\n",
    "\n",
    "# Display data\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcjoJKCFa2Tn"
   },
   "source": [
    "Now that we have these features, we can perform some EDA to better understand the data represented by these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_SuXicta2Tn",
    "outputId": "c1073fb9-0505-4714-effc-f7ff5d272b1d"
   },
   "outputs": [],
   "source": [
    "# First, let's define all the functions we will need for our calculations\n",
    "def get_percentage_with_feature(data, feature):\n",
    "    return (data[feature].sum()/data.shape[0]) * 100\n",
    "\n",
    "def get_avg_number_of_feature(data, feature):\n",
    "    return (data[feature].sum()/data.shape[0])\n",
    "\n",
    "# Now, let's use these functions to analyze our data\n",
    "print(\"Percentage of all training tweets with hashtags: {:.2f}%\".format(get_percentage_with_feature(train_data, \"has_hashtags\")))\n",
    "print(\"Percentage of all training tweets with mentions: {:.2f}%\".format(get_percentage_with_feature(train_data, \"has_mentions\")))\n",
    "print(\"Percentage of all training tweets with links: {:.2f}%\".format(get_percentage_with_feature(train_data, \"has_links\")))\n",
    "print(\"Percentage of all training tweets that are retweets: {:.2f}%\".format(get_percentage_with_feature(train_data, \"is_retweet\")))\n",
    "print(\"\\n\")\n",
    "\n",
    "disaster_train_data = train_data[train_data.target == 1]\n",
    "nondisaster_train_data = train_data[train_data.target == 0]\n",
    "\n",
    "print(\"Percentage of disaster tweets with hashtags: {:.2f}%\".format(get_percentage_with_feature(disaster_train_data, \"has_hashtags\")))\n",
    "print(\"Percentage of non-disaster tweets with hashtags: {:.2f}%\".format(get_percentage_with_feature(nondisaster_train_data, \"has_hashtags\")))\n",
    "\n",
    "print(\"Percentage of disaster tweets with mentions: {:.2f}%\".format(get_percentage_with_feature(disaster_train_data, \"has_mentions\")))\n",
    "print(\"Percentage of non-disaster tweets with mentions: {:.2f}%\".format(get_percentage_with_feature(nondisaster_train_data, \"has_mentions\")))\n",
    "\n",
    "print(\"Percentage of disaster tweets with links: {:.2f}%\".format(get_percentage_with_feature(disaster_train_data, \"has_links\")))\n",
    "print(\"Percentage of non-disaster tweets with links: {:.2f}%\".format(get_percentage_with_feature(nondisaster_train_data, \"has_links\")))\n",
    "\n",
    "print(\"Percentage of disaster tweets that are retweets: {:.2f}%\".format(get_percentage_with_feature(disaster_train_data, \"is_retweet\")))\n",
    "print(\"Percentage of non-disaster tweets that are retweets: {:.2f}%\".format(get_percentage_with_feature(nondisaster_train_data, \"is_retweet\")))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Average number of hashtags for disaster tweets: {:.2f}\".format(get_avg_number_of_feature(disaster_train_data, \"hashtag_count\")))\n",
    "print(\"Average number of hashtags for non-disaster tweets: {:.2f}\".format(get_avg_number_of_feature(nondisaster_train_data, \"hashtag_count\")))\n",
    "\n",
    "print(\"Average number of mentions for disaster tweets: {:.2f}\".format(get_avg_number_of_feature(disaster_train_data, \"mention_count\")))\n",
    "print(\"Average number of mentions for non-disaster tweets: {:.2f}\".format(get_avg_number_of_feature(nondisaster_train_data, \"mention_count\")))\n",
    "\n",
    "print(\"Average number of links for disaster tweets: {:.2f}\".format(get_avg_number_of_feature(disaster_train_data, \"link_count\")))\n",
    "print(\"Average number of links for non-disaster tweets: {:.2f}\".format(get_avg_number_of_feature(nondisaster_train_data, \"link_count\")))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qv3K22xa2Tn"
   },
   "source": [
    "From this initial EDA, we can see that there is a difference between disaster tweets and non-disaster tweets in terms of having links, mentions, and hashtags, with a marked difference for links in particular, leading us to believe that these features could provide value for us in modeling.\n",
    "\n",
    "To complete the EDA on these metadata features, we will also create plots to better understand the distributions of the feature count columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldzRDx8Ra2Tn",
    "outputId": "6af1cfe0-9f2f-4271-a28a-2a96946029f6"
   },
   "outputs": [],
   "source": [
    "def create_bar_plot(column, title):\n",
    "    df = train_data.copy(deep=True)\n",
    "    df['target'].replace({0: \"Non-Disaster\", 1: \"Disaster\"}, inplace=True)\n",
    "    df.pivot_table(index='target', columns=column, aggfunc='size').plot(kind='bar', figsize=(8, 8), rot=0, title=title, xlabel=\"Tweet Label\", ylabel=\"Count\")\n",
    "    plt.show()\n",
    "    \n",
    "create_bar_plot('link_count', 'Link Count Distribution for Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPa6Amfpa2To",
    "outputId": "0aa37245-2cf8-4f28-dd44-d0a12192940b"
   },
   "outputs": [],
   "source": [
    "create_bar_plot('mention_count', 'Mention Count Distribution for Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggEt2p3Ta2To",
    "outputId": "4fa32f6d-61c0-4e9f-d470-371000383b75"
   },
   "outputs": [],
   "source": [
    "create_bar_plot('hashtag_count', 'Hashtag Count Distribution for Training Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edeSnXdOa2To"
   },
   "source": [
    "From these plots above, we can see that there are differences in the distributions for our three count features, with a marked difference again in the link count that mirrors the difference in the binary feature for links that we saw in our earlier data analysis. With this EDA, it does appear that these metadata features, especially the link features, could add value to our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4Md3sfBKMH6"
   },
   "source": [
    "# Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYDUlQmRa2To"
   },
   "source": [
    "### Setting Up Data for Modeling\n",
    "\n",
    "To ensure that we are ready for modeling, we are going to split our train data into data and labels, set up a cross-validation procedure, and load in our test data. We want to set up cross-validation (CV) for our model, since we don't have all that much training data and also want to avoid overfitting. To do this, we will set up a CV function here that we can use both for GridSearchCV hyperparameter tuning and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNr-slqfa2To"
   },
   "outputs": [],
   "source": [
    "# Dropping all duplicate tweets based off of results of EDA\n",
    "train_data = train_data.drop_duplicates(subset=['text'], keep='first')\n",
    "\n",
    "# Creating numpy arrays for labels and tweets for modeling\n",
    "train_labels = np.array(train_data['target'])\n",
    "train_text = np.array(train_data['text'])\n",
    "\n",
    "# Dropping the label column from the train data\n",
    "train_data.drop(columns=[\"target\"])\n",
    "\n",
    "# Loading in the test data and creating numpy arrays for the text\n",
    "# Test data doesn't have labels so will not create array for that\n",
    "#test_data =  pd.read_csv('data/nlp-getting-started/test.csv')\n",
    "#test_text = np.array(test_data.text)\n",
    "\n",
    "# See more information about parameters here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "# \"n_splits\" splits our data into 10 training/validation sets\n",
    "# \"shuffle\" shuffles the data before splitting into batches, \"random_state\" allows for reproducability across function calls\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7aljnftKMH7"
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "In text classification problems, text pre-processing is a crucial part to prepping our data for analysis. This can be found in our text_clean function. Some pre-processing considerations we have made include:\n",
    "* removing numbers, symbols, and punctuation\n",
    "* standardizing to lowercase text\n",
    "* remove stop words\n",
    "* word stemming\n",
    "* trailing spaces\n",
    "* Lemmatize: (study -> study, studies -> study)\n",
    "* Stemmatize: (study -> study, studies -> studi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtRoVMVNKMH8"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data prior to running the model using this function\n",
    "# TODO: Here I've commented out the punctuation regex section\n",
    "# Feel free to uncomment that part BUT I think we should leave it out\n",
    "# altogether. See below for usage.\n",
    "\n",
    "def preprocess(text, method=None, tokenizer=sent_tokenize, rm_stop=False): \n",
    "    \"\"\"Returns a text processed string.\n",
    "\n",
    "    Arguments:\n",
    "    text      -- String, func is designed for loops\n",
    "    \n",
    "    method    -- ('s','l') Specify from s - stemmatize, l - lemmatize.\n",
    "                 None will mean you do not want to remove suffix.\n",
    "                 \n",
    "    tokenizer -- Any tokenizer function, from word to sentence to tweet.\n",
    "                 Tokenizer must not be an object.method unless you\n",
    "                 specifiy it to be like TweetTokenizer.tokenize.\n",
    "                 Sentence tokenizer is initialized here.\n",
    "                 \n",
    "    rm_stop   -- Bool. Remove stop words or not.\n",
    "    \"\"\"\n",
    "\n",
    "    #remove line breaks\n",
    "    text = re.sub(r\"\\n\",\"\",text)\n",
    "    #remove trailing spaces\n",
    "    text = re.sub(r'[ \\t]+$','', text)\n",
    "    #convert to lowercase \n",
    "    text = text.lower()\n",
    "    #remove digits and currencies \n",
    "    text = re.sub(r\"\\d+\",\"\",text) \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    #remove dates \n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    #remove non-ascii\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n",
    "    # Replacing all links with standard link\n",
    "    #text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text) \n",
    "    #text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(link_regex, \"http://t.co\", text)\n",
    "    # separate out mention symbol from text so that models can learn from number of mentions\n",
    "    p = re.compile(mention_regex)\n",
    "    text = p.sub(r'@ \\1',text)\n",
    "\n",
    "    # separate out hashtag symbol from hashtag so that models can learn from number of hashtags\n",
    "    q = re.compile(hashtag_regex)\n",
    "    text = q.sub(r'# \\1',text)\n",
    "    \n",
    "    # remove retweet indicator text as it's rarely used\n",
    "    text = re.sub(retweet_indicator, \"\", text)\n",
    "    \n",
    "    #remove punctuation\n",
    "    # Leave it? or talking point?!\n",
    "    #text = re.sub(r'[^\\w\\s]','',text)\n",
    "    \n",
    "    if rm_stop:\n",
    "        filtered_tokens = [word for word in tokenizer(text) \n",
    "                           if not word in set(stopwords.words('english'))]\n",
    "        text = \" \".join(filtered_tokens)\n",
    "        \n",
    "    if method == 'l':\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        lemm_tokens = [lemmer.lemmatize(word) \n",
    "                       for word in tokenizer(text)]\n",
    "        return \" \".join(lemm_tokens)\n",
    "    \n",
    "    elif method == 's':\n",
    "        porter = PorterStemmer()\n",
    "        stem_tokens = [porter.stem(word) \n",
    "                       for word in tokenizer(text)]\n",
    "        return \" \".join(stem_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LfGjcDpKMH9"
   },
   "source": [
    "# Usage\n",
    "### If someone wants to write a function that spits out all the possible model methods please do. This is to keep in mind that we are using K-folds CV to bag-> bootstrap our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JyS8JU-a2Tp",
    "outputId": "20d98f87-dc2d-45f4-8300-a0a4981083a4"
   },
   "outputs": [],
   "source": [
    "def run_model_on_preprocessed_text(preprocessed_text):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    transformed_data = tfidf.fit_transform(preprocessed_text)\n",
    "    model = MultinomialNB(alpha=0.9) # Best alpha from project 3\n",
    "    accuracy_scores = cross_val_score(model, transformed_data, train_labels, scoring='accuracy', cv=cv)\n",
    "    f1_scores = cross_val_score(model, transformed_data, train_labels, scoring='f1', cv=cv)\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(accuracy_scores), std(accuracy_scores)))\n",
    "    print('F1 Scores: %.3f (%.3f)' % (mean(f1_scores), std(f1_scores)))\n",
    "\n",
    "def preprocess_text():\n",
    "    return [preprocess(i) for i in train_data.text]\n",
    "    \n",
    "def preprocess_text_with_additional_cleaning(tokenizer):\n",
    "    return [preprocess(i,method='l',tokenizer=tokenizer,rm_stop=True) for i in train_data.text]\n",
    "\n",
    "print('Score on no SW removal, no suffix striping and TFIDF:')\n",
    "run_model_on_preprocessed_text(preprocess_text())\n",
    "print('Test on Lemmatize, remove stop and CountVectorize:')\n",
    "run_model_on_preprocessed_text(preprocess_text_with_additional_cleaning(word_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCjrqa-cKMH_"
   },
   "source": [
    "### Commented out the usage below because I (Aastha) changed the final proect to work with only one train set (because CV) and standardizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEVc_jU0KMH9"
   },
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# #df_ = df.sample(frac=1).reset_index()\n",
    "\n",
    "# # No stop word removal and no suffix cleaning. Naive Bayes example run\n",
    "# # I'll use CountVectorize in this to vectorize data.\n",
    "\n",
    "# df_ = df\n",
    "\n",
    "# # Process data in loop\n",
    "# processed_full = []\n",
    "# for i in df_.text:\n",
    "#     processed_full.append(preprocess(i))\n",
    "# df_.text = processed_full\n",
    "\n",
    "# # numtest = int(len(df_)/3.5)\n",
    "# # df_test = df_[:int(numtest/2)].reset_index(drop=True)\n",
    "# # df_dev = df_[int(numtest/2):numtest].reset_index(drop=True)\n",
    "# numtest = int(len(df_)/5)\n",
    "# df_train = df_[numtest:].reset_index(drop=True)\n",
    "# df_test = df_[:numtest].reset_index(drop=True) \n",
    "\n",
    "# train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "# dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "# test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "# # Naive Bayes example run\n",
    "# # I'll use TF-IDF in this to vectorize data.\n",
    "\n",
    "# tfidf = TfidfVectorizer()\n",
    "# t_data = tfidf.fit_transform(train_data)\n",
    "# dt_data = tfidf.transform(dev_data)\n",
    "# tt_data = tfidf.transform(test_data)\n",
    "# m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label)\n",
    "# pred = m_nb.predict(dt_data)\n",
    "# pred_test = m_nb.predict(tt_data)\n",
    "# print('Score on no SW removal, no suffix striping and TFIDF:')\n",
    "# print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "# print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))\n",
    "# print()\n",
    "\n",
    "\n",
    "# # Lemmatization, no stop words removal, Naive Bayes example run\n",
    "# # I'll use CountVectorize in this to vectorize data.\n",
    "\n",
    "# tokenizer = word_tokenize\n",
    "# df_ = df\n",
    "\n",
    "# processed_full = []\n",
    "# for i in df_.text:\n",
    "#     processed_full.append(preprocess(i,method='l',tokenizer=tokenizer,rm_stop=True))\n",
    "# df_.text = processed_full\n",
    "\n",
    "# numtest = int(len(df_)/5)\n",
    "# df_train = df_[numtest:].reset_index(drop=True)\n",
    "# df_test = df_[:numtest].reset_index(drop=True)\n",
    "\n",
    "# train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "# dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "# test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "\n",
    "# tfidf = CountVectorizer(ngram_range=(1,1))\n",
    "# t_data = tfidf.fit_transform(train_data)\n",
    "# dt_data = tfidf.transform(dev_data)\n",
    "# tt_data = tfidf.transform(test_data)\n",
    "# m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "# pred = m_nb.predict(dt_data)\n",
    "# pred_test = m_nb.predict(tt_data)\n",
    "# print()\n",
    "# print('Test on Lemmatize, remove stop and CountVectorize:')\n",
    "# print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "# print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWjsfNB6KMH-"
   },
   "source": [
    "#### Clean the data and strip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiGAzXgYKMH-"
   },
   "source": [
    "### After the data has been cleaned and text pre-processed, we can begin exploring different algorithms. The three machine learning algorithms we will focus on are:\n",
    "* Naive Bayes\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* KMeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Code for Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def run_model_on_transformed_data(model, transformed_data, params):\n",
    "    print(\"Fitting GridSearch to optimize accuracy (this takes a while)...\")\n",
    "    accuracy_gridsearch_model = fit_gridsearch_model(model, transformed_data, params, 'accuracy')\n",
    "    print(\"Fitting GridSearch to optimize f1 score (this takes a while)...\")\n",
    "    f1_gridsearch_model = fit_gridsearch_model(model, transformed_data, params, 'f1')\n",
    "\n",
    "    print('Best params for accuracy: ', accuracy_gridsearch_model.best_params_)\n",
    "    print('Best mean score for accuracy: ', accuracy_gridsearch_model.best_score_)\n",
    "\n",
    "    print('Best params for f1: ', f1_gridsearch_model.best_params_)\n",
    "    print('Best mean score for f1: ', f1_gridsearch_model.best_score_)\n",
    "    \n",
    "def fit_gridsearch_model(model, data, params, scoring):\n",
    "    gridsearch_model = GridSearchCV(model, param_grid=params, cv=cv, scoring=scoring, n_jobs=-1, verbose=3)\n",
    "    return gridsearch_model.fit(data, train_labels)\n",
    "\n",
    "def transform_data_with_count_vectorizer(preprocessed_text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(preprocessed_text)\n",
    "       \n",
    "def transform_data_with_tfidf(preprocessed_text):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    return tfidf.fit_transform(preprocessed_text)\n",
    "\n",
    "preprocessed_text = preprocess_text()\n",
    "preprocessed_text_with_additional_cleaning = preprocess_text_with_additional_cleaning(word_tokenize)\n",
    "\n",
    "tfidf_transformed = transform_data_with_tfidf(preprocessed_text)\n",
    "count_vectorizer_transformed = transform_data_with_count_vectorizer(preprocessed_text)\n",
    "\n",
    "tfidf_transformed_cleaned = transform_data_with_tfidf(preprocessed_text_with_additional_cleaning)\n",
    "count_vectorizer_transformed_cleaned = transform_data_with_count_vectorizer(preprocessed_text_with_additional_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizer with default pre-process\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "text_columns = pd.DataFrame(vectorizer.fit_transform(train_data['text']).todense(), columns = vectorizer.get_feature_names())\n",
    "\n",
    "#Vectorizer with defined preprocess\n",
    "vectorizer_pp = CountVectorizer(preprocessor = preprocess, stop_words='english')\n",
    "text_columns_pp = pd.DataFrame(vectorizer_pp.fit_transform(train_data['text']).todense(), columns = vectorizer_pp.get_feature_names())\n",
    "\n",
    "#Tfidf Vectorizer\n",
    "vectorizer_Tf = TfidfVectorizer()\n",
    "text_columns_Tf = pd.DataFrame(vectorizer_Tf.fit_transform(train_data['text']).todense(), columns = vectorizer_Tf.get_feature_names())\n",
    "\n",
    "\n",
    "#Extract feature columns\n",
    "#NOTE: For Multinomial NB, should use counts rather than attribute\n",
    "feature_columns = train_data[['mention_count', 'hashtag_count', 'link_count']]\n",
    "\n",
    "X = np.hstack((feature_columns, text_columns))\n",
    "X_pp = np.hstack((feature_columns, text_columns_pp))\n",
    "X_Tf = np.hstack((feature_columns, text_columns_Tf))\n",
    "\n",
    "#Alpha values to test (Okay to test along single parameter in NB)\n",
    "alphas = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "\n",
    "scores_acc_v = []\n",
    "scores_f1_v = []\n",
    "\n",
    "scores_acc_v_pp = []\n",
    "scores_f1_v_pp = []\n",
    "\n",
    "scores_acc_v_Tf = []\n",
    "scores_f1_v_Tf = []\n",
    "\n",
    "for a in alphas:\n",
    "   \n",
    "    NB_base = MultinomialNB(alpha = a)\n",
    "   \n",
    "    #Fit for default\n",
    "    NB_base.fit(X,train_labels)\n",
    "    scores_acc_v.append(mean(cross_val_score(NB_base, X, train_labels, cv=cv, scoring='accuracy')))\n",
    "    scores_f1_v.append(mean(cross_val_score(NB_base, X, train_labels, cv=cv, scoring='f1')))\n",
    "    print(str(a) + \" alpha, default preprocess: \" + str(scores_acc_v[-1]) + ', ' + str(scores_f1_v[-1]))\n",
    "   \n",
    "    #Fit for preprocess\n",
    "    NB_base.fit(X_pp,train_labels)\n",
    "    scores_acc_v_pp.append(mean(cross_val_score(NB_base, X_pp, train_labels, cv=cv, scoring='accuracy')))\n",
    "    scores_f1_v_pp.append(mean(cross_val_score(NB_base, X_pp, train_labels, cv=cv, scoring='f1')))\n",
    "    print(str(a) + \" alpha, base preprocess: \" + str(scores_acc_v_pp[-1]) + ', ' + str(scores_f1_v_pp[-1]))\n",
    "   \n",
    "     #Fit for preprocess\n",
    "    NB_base.fit(X_Tf,train_labels)\n",
    "    scores_acc_v_Tf.append(mean(cross_val_score(NB_base, X_Tf, train_labels, cv=cv, scoring='accuracy')))\n",
    "    scores_f1_v_Tf.append(mean(cross_val_score(NB_base, X_Tf, train_labels, cv=cv, scoring='f1')))\n",
    "    print(str(a) + \" alpha, Tfidf: \" + str(scores_acc_v_Tf[-1]) + ', ' + str(scores_f1_v_Tf[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK9RJovIa-7N"
   },
   "source": [
    "## Logistic Regression Model Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THBzU8F2a6xt"
   },
   "outputs": [],
   "source": [
    "#CREDIT TO SOURCES: \n",
    "#https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "#https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/\n",
    "#https://towardsdatascience.com/natural-language-processing-on-multiple-columns-in-python-554043e05308\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#drop duplicates\n",
    "train_data = train_data.drop_duplicates(subset=['text'], keep='first')\n",
    "train_labels = train_data.target\n",
    "\n",
    "#vectorize data\n",
    "vectorizer = CountVectorizer(preprocessor=text_clean, stop_words='english')\n",
    "vtrain = vectorizer.fit_transform(train_data.text)\n",
    "text_train = pd.DataFrame(vtrain.todense(), columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "#generate TfidfVectorizer data\n",
    "tfid_vectorizer = TfidfVectorizer()\n",
    "t_train = tfid_vectorizer.fit_transform(train_data.text)\n",
    "tfid_text_train = pd.DataFrame(t_train.todense(), columns = tfid_vectorizer.get_feature_names_out())\n",
    "\n",
    "#determine additional features to include in model \n",
    "features = ['is_retweet', 'has_mentions', 'has_links']\n",
    "features_train = train_data[features]\n",
    "\n",
    "#use hstack to combine CountVectorizer and addl feature data \n",
    "train = np.hstack((features_train, text_train))\n",
    "\n",
    "#use hstack to combine TfidfVectorizer and addl feature data \n",
    "tfid_train = np.hstack((features_train, tfid_text_train))\n",
    "\n",
    "def log_model(train, labels):\n",
    "  #define c values for testing\n",
    "  c_values = [0.001,0.01,0.1, 0.5, 1,10,100]\n",
    "  accuracy_score_list = []\n",
    "  f1_score_list = []\n",
    "\n",
    "  #build model\n",
    "  cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "  for c in c_values: \n",
    "    model = LogisticRegression(C=c, solver=\"liblinear\", multi_class=\"auto\")\n",
    "    accuracy_score = cross_val_score(model, train, labels, cv=10, scoring='accuracy')\n",
    "    accuracy_score_list.append(accuracy_score.mean())\n",
    "    f1_score = cross_val_score(model, train, labels, cv=10, scoring='f1')\n",
    "    f1_score_list.append(f1_score.mean())\n",
    "\n",
    "  #convert lists to arrays\n",
    "  accuracy_scores = np.array(accuracy_score_list)\n",
    "  f1_scores = np.array(accuracy_score_list)\n",
    "\n",
    "  #identify max scores\n",
    "  max_accuracy_score = np.where(accuracy_score_list == max(accuracy_score_list))\n",
    "  max_f1_score = np.where(f1_score_list == max(f1_score_list))\n",
    "\n",
    "  #print max scores\n",
    "  string = \"F1 score of {:.2%}\".format(f1_scores[max_f1_score[0][0]]), \"with a C =\" , c_values[max_f1_score[0][0]], \"and the highest accuracy score of {:.2%}\".format(accuracy_scores[max_accuracy_score[0][0]]), \"with C =\", c_values[max_accuracy_score[0][0]], \".\"\n",
    "  return string\n",
    "\n",
    "print(\"The model using CountVectorizer had the highest\", *log_model(train, train_labels))\n",
    "print(\"The model using TfidfVectorizer had the highest\", *log_model(tfid_train, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, preprocessing\n",
    "\n",
    "# Function to scale data to speed up SVM fitting\n",
    "def scale_data(vectorized_text):\n",
    "    return preprocessing.MaxAbsScaler().fit_transform(vectorized_text)\n",
    "\n",
    "params = {\n",
    "    'C': [.01, .1, .5, 1],\n",
    "    'kernel': ['linear'],\n",
    "}\n",
    "\n",
    "model = svm.SVC(random_state=1)\n",
    "\n",
    "print('No SW removal, no suffix striping, TFIDF:')\n",
    "run_model_on_transformed_data(model, scale_data(tfidf_transformed), params)\n",
    "print('\\n')\n",
    "\n",
    "print('No SW removal, no suffix striping, CountVectorize:')\n",
    "run_model_on_transformed_data(model, scale_data(count_vectorizer_transformed), params)\n",
    "print('\\n')\n",
    "\n",
    "print('Lemmatize, remove stop words, TFIDF:')\n",
    "run_model_on_transformed_data(model, scale_data(tfidf_transformed_cleaned), params)\n",
    "print('\\n')\n",
    "\n",
    "print('Lemmatize, remove stop words, CountVectorize:')\n",
    "run_model_on_transformed_data(model, scale_data(count_vectorizer_transformed_cleaned), params)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Final Project.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "common-cpu.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
