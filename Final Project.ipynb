{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets: Project Intro\n",
    "### For this project, we will be using this data set of twitter post text, found on kaggle. The goal is to come up with an algorithm that most accuractely classifies a tweet as indicative of a real disaster or not a real disaster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import *\n",
    "from sklearn import metrics\n",
    "\n",
    "# SK-learn Decomp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# NLP processors\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: this package nltk.download(wordnet) is for lemmatize to work\n",
    "# The download did not work so we have to use the below code.. lol\n",
    "\n",
    "# Please run this code and SSL will pop up a window for you to choose a\n",
    "# Module to download, click 'all packages' and scroll down\n",
    "# and select 'wordnet'. You only need to download it once. \n",
    "# Not tested in Google Colab :3\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will read in the \"train\" dataset, which contains the correct labels. We will split the data 50/50 between train and dev (or test). For ease of analysis and text processing, the data will be further split into \"pos\" (label = 1) and \"neg\" (label = 0) dataframes and text only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some data metrics\n",
      "\n",
      "Shape of train data: (5438, 5)\n",
      "\n",
      "Missing data in each column:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "Number of disaster tweets:\n",
      "0    2996\n",
      "1    2442\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read in data\n",
    "# df = pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv')\n",
    "df = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "# sample the data, acts as shuffling the data on row\n",
    "\n",
    "#50/50 split between train and dev\n",
    "# allocate more for traiing if we do it this way, i'll run some\n",
    "# analysis to see if my cluster bootstrap can imrpove the models we run.\n",
    "numtest = int(len(df)/3.5)\n",
    "df_train = df[numtest:].reset_index(drop=True)\n",
    "df_test = df[:int(numtest/2)].reset_index(drop=True)\n",
    "df_dev = df[int(numtest/2):numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = df_train.text, df_train.target\n",
    "dev_data, dev_label = df_dev.text, df_dev.target\n",
    "test_data, test_label = df_test.text, df_test.target\n",
    "\n",
    "#split into disaster and non disaster data\n",
    "df_neg = df_train.loc[df_train.target == 0]\n",
    "df_pos = df_train.loc[df_train.target == 1]\n",
    "\n",
    "#split into disaster and nondisaster tweets only\n",
    "neg_text = df_neg.text\n",
    "pos_text = df_pos.text\n",
    "\n",
    "print(\"Some data metrics\\n\")\n",
    "print(\"Shape of train data:\", df_train.shape)\n",
    "print(\"\\nMissing data in each column:\\n\" + str(df.isnull().sum()))\n",
    "print(\"\\nNumber of disaster tweets:\\n\"+ str(train_label.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because the available tags, keyword and location, are sparse and method of construction are unclear to us, we wanted to create new tags for the text that we may be able to train later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id             keyword        location  \\\n",
      "0  8367                ruin         Belfast   \n",
      "1  4164               drown             NaN   \n",
      "2  9232   suicide%20bombing             NaN   \n",
      "3  7587            outbreak  Fukuoka, Japan   \n",
      "4  8816              sirens       Hollywood   \n",
      "5  9673             tornado         Midwest   \n",
      "6  2538           collision             NaN   \n",
      "7   798              battle             NaN   \n",
      "8  7108            military          Alaska   \n",
      "9  7228  natural%20disaster             NaN   \n",
      "\n",
      "                                                text  target  \\\n",
      "0  And then I go a ruin it all with something awf...       0   \n",
      "1        @GraysonDolan only if u let me drown you ??       0   \n",
      "2  meek mill should join isis since he loves suic...       0   \n",
      "3  Families to sue over Legionnaires: More than 4...       1   \n",
      "4  @TravelElixir Any idea what's going on? I hear...       1   \n",
      "5  (OK)  Severe Thunderstorm Warning issued Augus...       1   \n",
      "6  my favorite lady came to our volunteer meeting...       1   \n",
      "7  Dragon Ball Z: Battle Of Gods (2014) - Rotten ...       0   \n",
      "8  Mike Magner Discusses A Trust Betrayed: http:/...       0   \n",
      "9  @ConnorFranta #AskConnor if you were a natural...       0   \n",
      "\n",
      "                             hashtag          mentions links  retweet  \\\n",
      "0                          [minions]                []   [t]    False   \n",
      "1                                 []    [GraysonDolan]    []    False   \n",
      "2                                 []                []    []    False   \n",
      "3              [News, check, follow]                []   [t]    False   \n",
      "4                                 []    [TravelElixir]    []    False   \n",
      "5                             [okwx]                []   [t]    False   \n",
      "6                                 []                []   [t]    False   \n",
      "7                                 []  [RottenTomatoes]   [t]    False   \n",
      "8  [military, veterans, environment]         [YouTube]   [t]    False   \n",
      "9                        [AskConnor]    [ConnorFranta]    []    False   \n",
      "\n",
      "   mentions_ind  hashtag_ind  links_ind  retweet_ind  \n",
      "0             0            1          1            0  \n",
      "1             1            0          0            0  \n",
      "2             0            0          0            0  \n",
      "3             0            1          1            0  \n",
      "4             1            0          0            0  \n",
      "5             0            1          1            0  \n",
      "6             0            0          1            0  \n",
      "7             1            0          1            0  \n",
      "8             1            1          1            0  \n",
      "9             1            1          0            0  \n"
     ]
    }
   ],
   "source": [
    "df_train['hashtag'] = df_train['text'].apply(lambda s: re.findall(r'#(\\w+)', s))\n",
    "df_train['mentions'] = df_train['text'].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "df_train['links'] = df_train['text'].apply(lambda x: re.findall(r\"http:\\/\\/(\\w+)\", x))\n",
    "df_train['retweet'] = df_train['text'].apply(lambda x: \"rt\" in x.lower().split())\n",
    "\n",
    "df_train['mentions_ind'] = df_train.mentions.apply(lambda y: 0 if len(y)==0 else 1)\n",
    "df_train['hashtag_ind'] = df_train.hashtag.apply(lambda y: 0 if len(y)==0 else 1)\n",
    "df_train['links_ind'] = df_train.links.apply(lambda y: 0 if len(y)==0 else 1)\n",
    "df_train['retweet_ind'] = df_train.retweet.apply(lambda y: 0 if y == False else 1)\n",
    "\n",
    "print(df_train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text classification problems, text pre-processing is a crucial part to prepping our data for analysis. This can be found in our text_clean function. Some pre-processing considerations we have made include:\n",
    "* removing numbers, symbols, and punctuation\n",
    "* standardizing to lowercase text\n",
    "* remove stop words\n",
    "* word stemming\n",
    "* trailing spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "* Lemmatize: (study -> study, studies -> study)\n",
    "* Stemmatize: (study -> study, studies -> studi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data prior to running the model using this function\n",
    "# TODO: Here I've commented out the punctuation regex section\n",
    "# Feel free to uncomment that part BUT I think we should leave it out\n",
    "# altogether. See below for usage.\n",
    "\n",
    "def preprocess(text, method=None, tokenizer=sent_tokenize, rm_stop=False): \n",
    "    \"\"\"Returns a text processed string.\n",
    "\n",
    "    Arguments:\n",
    "    text      -- String, func is designed for loops\n",
    "    \n",
    "    method    -- ('s','l') Specify from s - stemmatize, l - lemmatize.\n",
    "                 None will mean you do not want to remove suffix.\n",
    "                 \n",
    "    tokenizer -- Any tokenizer function, from word to sentence to tweet.\n",
    "                 Tokenizer must not be an object.method unless you\n",
    "                 specifiy it to be like TweetTokenizer.tokenize.\n",
    "                 Sentence tokenizer is initialized here.\n",
    "                 \n",
    "    rm_stop   -- Bool. Remove stop words or not.\n",
    "    \"\"\"\n",
    "\n",
    "    #remove line breaks\n",
    "    text = re.sub(r\"\\n\",\"\",text)\n",
    "    # remove trailing spaces\n",
    "    text = re.sub(r'[ \\t]+$','', text)\n",
    "    #convert to lowercase \n",
    "    text = text.lower()\n",
    "    #remove digits and currencies \n",
    "    text = re.sub(r\"\\d+\",\"\",text) \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    #remove dates \n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    #remove non-ascii\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n",
    "    #remove hyperlinks\n",
    "    #text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    #remove punctuation\n",
    "    # Leave it? or talking point?!\n",
    "    #text = re.sub(r'[^\\w\\s]','',text)\n",
    "    \n",
    "    if rm_stop:\n",
    "        filtered_tokens = [word for word in tokenizer(text) \n",
    "                           if not word in set(stopwords.words('english'))]\n",
    "        text = \" \".join(filtered_tokens)\n",
    "        \n",
    "    if method == 'l':\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        lemm_tokens = [lemmer.lemmatize(word) \n",
    "                       for word in tokenizer(text)]\n",
    "        return \" \".join(lemm_tokens)\n",
    "    \n",
    "    elif method == 's':\n",
    "        porter = PorterStemmer()\n",
    "        stem_tokens = [porter.stem(word) \n",
    "                       for word in tokenizer(text)]\n",
    "        return \" \".join(stem_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "### If someone wants to write a function that spits out all the possible model methods please do. This is to keep in mind that we are using K-folds CV to bag-> bootstrap our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on no SW removal, no suffix striping and TFIDF:\n",
      "F1 Score: 0.7702\n",
      "Accuracy: 0.7720\n",
      "\n",
      "\n",
      "Test on Lemmatize, remove stop and CountVectorize:\n",
      "F1 Score: 0.7676\n",
      "Accuracy: 0.7661\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "#df_ = df.sample(frac=1).reset_index()\n",
    "\n",
    "# No stop word removal and no suffix cleaning. Naive Bayes example run\n",
    "# I'll use CountVectorize in this to vectorize data.\n",
    "\n",
    "df_ = df\n",
    "\n",
    "# Process data in loop\n",
    "processed_full = []\n",
    "for i in df_.text:\n",
    "    processed_full.append(preprocess(i))\n",
    "df_.text = processed_full\n",
    "\n",
    "# numtest = int(len(df_)/3.5)\n",
    "# df_test = df_[:int(numtest/2)].reset_index(drop=True)\n",
    "# df_dev = df_[int(numtest/2):numtest].reset_index(drop=True)\n",
    "numtest = int(len(df_)/5)\n",
    "df_train = df_[numtest:].reset_index(drop=True)\n",
    "df_test = df_[:numtest].reset_index(drop=True) \n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "# Naive Bayes example run\n",
    "# I'll use TF-IDF in this to vectorize data.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label)\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_test = m_nb.predict(tt_data)\n",
    "print('Score on no SW removal, no suffix striping and TFIDF:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))\n",
    "print()\n",
    "\n",
    "\n",
    "# Lemmatization, no stop words removal, Naive Bayes example run\n",
    "# I'll use CountVectorize in this to vectorize data.\n",
    "\n",
    "tokenizer = word_tokenize\n",
    "df_ = df\n",
    "\n",
    "processed_full = []\n",
    "for i in df_.text:\n",
    "    processed_full.append(preprocess(i,method='l',tokenizer=tokenizer,rm_stop=True))\n",
    "df_.text = processed_full\n",
    "\n",
    "numtest = int(len(df_)/5)\n",
    "df_train = df_[numtest:].reset_index(drop=True)\n",
    "df_test = df_[:numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "\n",
    "tfidf = CountVectorizer(ngram_range=(1,1))\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_test = m_nb.predict(tt_data)\n",
    "print()\n",
    "print('Test on Lemmatize, remove stop and CountVectorize:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data and strip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the data has been cleaned and text pre-processed, we can begin exploring different algorithms. The three machine learning algorithms we will focus on are:\n",
    "* Naive Bayes\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
