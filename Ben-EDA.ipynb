{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acbe2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import *\n",
    "from sklearn import metrics\n",
    "\n",
    "# SK-learn Decomp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# NLP processors\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4120c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splits\n",
    "\n",
    "dat = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "test_      = pd.read_csv('data/nlp-getting-started/test.csv')\n",
    "\n",
    "div        = int(len(test_)/2)\n",
    "test_data  = test_[div:]\n",
    "dev_data   = test_[:div]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc8bf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>5500</td>\n",
       "      <td>flames</td>\n",
       "      <td>Fargo, ND</td>\n",
       "      <td>You were just waiting for us to go down in flames</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>5504</td>\n",
       "      <td>flames</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is gonna go down in flames ?? https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>5508</td>\n",
       "      <td>flames</td>\n",
       "      <td>Here, There &amp; Everywhere..</td>\n",
       "      <td>I'm listening to 'Zion' by Flames on #Pandora ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>5511</td>\n",
       "      <td>flattened</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think that's been the best bit of this match...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>5512</td>\n",
       "      <td>flattened</td>\n",
       "      <td>support all girls!</td>\n",
       "      <td>why would anyone want to hear some type of shi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword                    location  \\\n",
       "1631  5500     flames                   Fargo, ND   \n",
       "1632  5504     flames                         NaN   \n",
       "1633  5508     flames  Here, There & Everywhere..   \n",
       "1634  5511  flattened                         NaN   \n",
       "1635  5512  flattened          support all girls!   \n",
       "\n",
       "                                                   text  \n",
       "1631  You were just waiting for us to go down in flames  \n",
       "1632  This is gonna go down in flames ?? https://t.c...  \n",
       "1633  I'm listening to 'Zion' by Flames on #Pandora ...  \n",
       "1634  I think that's been the best bit of this match...  \n",
       "1635  why would anyone want to hear some type of shi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59efe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster tweets:  3271 \n",
      "not disaster tweets:  4342 \n",
      "disaster tweets w/ kw:  3229 \n",
      "not disaster tweets w/ kw:  4323\n"
     ]
    }
   ],
   "source": [
    "num_disaster = len(dat[dat['target']==1])\n",
    "num_nodisaster = len(dat[dat['target']==0])\n",
    "num_disaster_kw =  len(dat[(dat.target == 1) & (~dat.keyword.isnull())])\n",
    "num_nd_kw =  len(dat[(dat.target == 0) & (~dat.keyword.isnull())])\n",
    "print('disaster tweets: ',num_disaster, '\\nnot disaster tweets: ',num_nodisaster\n",
    "      ,'\\ndisaster tweets w/ kw: ', num_disaster_kw, '\\nnot disaster tweets w/ kw: ', num_nd_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97838fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers \n",
    "def sort(d,r):\n",
    "    return sorted(d.items(),key=lambda item:item[1],reverse=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24466f40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-77faf803e5ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfirst_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfirst_words_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msorted_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_words_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msorted_fw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# * to unpack tuple, zip to split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "first_word = train_data.text.str.split(expand=True)[0]\n",
    "first_words_c = dict(collections.Counter(first_word))\n",
    "sorted_fw = sort(first_words_c,True)\n",
    "\n",
    "x, y = zip(*sorted_fw[0:9]) # * to unpack tuple, zip to split \n",
    "plt.plot(x,y)\n",
    "plt.title('Top 10 First Words in Data')\n",
    "plt.xlabel('First Word/Token')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbfcfa0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>80</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>TRUCK ABLAZE : R21. VOORTREKKER AVE. OUTSIDE O...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>109</td>\n",
       "      <td>accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @SleepJunkies: Sleeping pills double your r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>128</td>\n",
       "      <td>accident</td>\n",
       "      <td>New Hanover County, NC</td>\n",
       "      <td>FYI CAD:FYI: ;ACCIDENT PROPERTY DAMAGE;NHS;999...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>129</td>\n",
       "      <td>accident</td>\n",
       "      <td>Maldives</td>\n",
       "      <td>RT nAAYf: First accident in years. Turning ont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>10650</td>\n",
       "      <td>wounds</td>\n",
       "      <td>Earth</td>\n",
       "      <td>RT @DianneG: Gunshot wound #9 is in the bicep....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7475</th>\n",
       "      <td>10692</td>\n",
       "      <td>wreck</td>\n",
       "      <td>Lebanon, Tennessee</td>\n",
       "      <td>Watertown Gazette owner charged in wreck http:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7510</th>\n",
       "      <td>10743</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>WorldWide</td>\n",
       "      <td>#Australia #News ; RT janeenorman: 'High proba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7518</th>\n",
       "      <td>10751</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>CIA plot! *rolling eyes* RT @ajabrown: Chinese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>10851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   keyword                       location  \\\n",
       "32       49    ablaze  Est. September 2012 - Bristol   \n",
       "56       80    ablaze                   South Africa   \n",
       "75      109  accident                            NaN   \n",
       "86      128  accident         New Hanover County, NC   \n",
       "87      129  accident                       Maldives   \n",
       "...     ...       ...                            ...   \n",
       "7442  10650    wounds                          Earth   \n",
       "7475  10692     wreck             Lebanon, Tennessee   \n",
       "7510  10743  wreckage                      WorldWide   \n",
       "7518  10751  wreckage                        Bangkok   \n",
       "7596  10851       NaN                            NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "32    We always try to bring the heavy. #metal #RT h...       0  \n",
       "56    TRUCK ABLAZE : R21. VOORTREKKER AVE. OUTSIDE O...       1  \n",
       "75    RT @SleepJunkies: Sleeping pills double your r...       0  \n",
       "86    FYI CAD:FYI: ;ACCIDENT PROPERTY DAMAGE;NHS;999...       1  \n",
       "87    RT nAAYf: First accident in years. Turning ont...       1  \n",
       "...                                                 ...     ...  \n",
       "7442  RT @DianneG: Gunshot wound #9 is in the bicep....       0  \n",
       "7475  Watertown Gazette owner charged in wreck http:...       1  \n",
       "7510  #Australia #News ; RT janeenorman: 'High proba...       1  \n",
       "7518  CIA plot! *rolling eyes* RT @ajabrown: Chinese...       1  \n",
       "7596  RT @LivingSafely: #NWS issues Severe #Thunders...       1  \n",
       "\n",
       "[153 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the RT in tweets, as this is commonly used as 'retweet' by twitter users\n",
    "dat[dat.text.str.contains('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fb9490e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For argument \"inplace\" expected type bool, received type str.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-56c4ba484da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'body%20bags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/berkeley/berkeley/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, inplace, how)\u001b[0m\n\u001b[1;32m   4863\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4864\u001b[0m         \"\"\"\n\u001b[0;32m-> 4865\u001b[0;31m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4866\u001b[0m         \u001b[0;31m# Validate the axis parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4867\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/berkeley/berkeley/lib/python3.7/site-packages/pandas/util/_validators.py\u001b[0m in \u001b[0;36mvalidate_bool_kwarg\u001b[0;34m(value, arg_name, none_allowed, int_allowed)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgood_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         raise ValueError(\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;34mf'For argument \"{arg_name}\" expected type bool, received '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;34mf\"type {type(value).__name__}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: For argument \"inplace\" expected type bool, received type str."
     ]
    }
   ],
   "source": [
    "dat[dat.keyword.dropna(inplace='n').str.contains('body%20bags')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22f05bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>10853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>10854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>10859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>10867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "5         8     NaN      NaN   \n",
       "6        10     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7598  10853     NaN      NaN   \n",
       "7599  10854     NaN      NaN   \n",
       "7601  10859     NaN      NaN   \n",
       "7604  10863     NaN      NaN   \n",
       "7607  10867     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "5     #RockyFire Update => California Hwy. 20 closed...       1  \n",
       "6     #flood #disaster Heavy rain causes flash flood...       1  \n",
       "...                                                 ...     ...  \n",
       "7598  Father-of-three Lost Control of Car After Over...       1  \n",
       "7599  1.3 #Earthquake in 9Km Ssw Of Anza California ...       1  \n",
       "7601  #breaking #LA Refugio oil spill may have been ...       1  \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...       1  \n",
       "7607  #stormchase Violent Record Breaking EF-5 El Re...       1  \n",
       "\n",
       "[1761 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We would also remove # hashtags from the tweets texts, maybe even consider @\n",
    "dat[dat.text.str.contains('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85a37aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>55</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>World Wide!!</td>\n",
       "      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>10867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3971 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword                       location  \\\n",
       "31       48  ablaze                     Birmingham   \n",
       "32       49  ablaze  Est. September 2012 - Bristol   \n",
       "33       50  ablaze                         AFRICA   \n",
       "35       53  ablaze                     London, UK   \n",
       "37       55  ablaze                   World Wide!!   \n",
       "...     ...     ...                            ...   \n",
       "7606  10866     NaN                            NaN   \n",
       "7607  10867     NaN                            NaN   \n",
       "7608  10869     NaN                            NaN   \n",
       "7610  10871     NaN                            NaN   \n",
       "7612  10873     NaN                            NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "32    We always try to bring the heavy. #metal #RT h...       0  \n",
       "33    #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "37    INEC Office in Abia Set Ablaze - http://t.co/3...       1  \n",
       "...                                                 ...     ...  \n",
       "7606  Suicide bomber kills 15 in Saudi security site...       1  \n",
       "7607  #stormchase Violent Record Breaking EF-5 El Re...       1  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[3971 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should remove all feature sets that begin with HTTP, since they are links to other sites, causes noise\n",
    "dat[dat.text.str.contains('http')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9aad92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this shit for now\n",
    "\n",
    "stemmer = PorterStemmer().stem\n",
    "tokenize = nltk.word_tokenize\n",
    "\n",
    "def stem(tokens,stemmer = PorterStemmer().stem):\n",
    "    return [stemmer(w.lower()) for w in tokens] \n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Extract simple lemmas based on tokenization and stemming\n",
    "    Input: string\n",
    "    Output: list of strings (lemmata)\n",
    "    \"\"\"\n",
    "    return stem(tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7126db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some data metrics\n",
      "\n",
      "Shape of train data: (5438, 5)\n",
      "\n",
      "Missing data in each column:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "Number of disaster tweets:\n",
      "0    2996\n",
      "1    2442\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read in data\n",
    "# df = pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv')\n",
    "df = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "# sample the data, acts as shuffling the data on row\n",
    "\n",
    "#50/50 split between train and dev\n",
    "# allocate more for traiing if we do it this way, i'll run some\n",
    "# analysis to see if my cluster bootstrap can imrpove the models we run.\n",
    "numtest = int(len(df)/3.5)\n",
    "df_train = df[numtest:].reset_index(drop=True)\n",
    "df_test = df[:int(numtest/2)].reset_index(drop=True)\n",
    "df_dev = df[int(numtest/2):numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = df_train.text, df_train.target\n",
    "dev_data, dev_label = df_dev.text, df_dev.target\n",
    "test_data, test_label = df_test.text, df_test.target\n",
    "\n",
    "#split into disaster and non disaster data\n",
    "df_neg = df_train.loc[df_train.target == 0]\n",
    "df_pos = df_train.loc[df_train.target == 1]\n",
    "\n",
    "#split into disaster and nondisaster tweets only\n",
    "neg_text = df_neg.text\n",
    "pos_text = df_pos.text\n",
    "\n",
    "print(\"Some data metrics\\n\")\n",
    "print(\"Shape of train data:\", df_train.shape)\n",
    "print(\"\\nMissing data in each column:\\n\" + str(df.isnull().sum()))\n",
    "print(\"\\nNumber of disaster tweets:\\n\"+ str(train_label.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b37cfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, method=None, tokenizer=None):\n",
    "    #remove line breaks\n",
    "    text = re.sub(r\"\\n\",\"\",text)\n",
    "\n",
    "    #convert to lowercase \n",
    "    text = text.lower()\n",
    "\n",
    "    #remove digits and currencies \n",
    "    text = re.sub(r\"\\d+\",\"\",text) \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)      \n",
    "\n",
    "    #remove dates \n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)\n",
    "\n",
    "    #remove non-ascii\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n",
    "\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "\n",
    "    #remove hyperlinks\n",
    "    #text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # remove trailing spaces\n",
    "    text = re.sub(r'[ \\t]+$','', text)\n",
    "    \n",
    "    if method == 'l':\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        lemm_tokens = [lemmer.lemmatize(word) for word in tokenizer(text)]\n",
    "        return \" \".join(lemm_tokens)\n",
    "    \n",
    "    elif method == 's':\n",
    "        porter = PorterStemmer()\n",
    "        stem_tokens = [porter.stem(word) for word in tokenizer(text)]\n",
    "        return \" \".join(stem_tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "    # remove stop words, yea don't remove stop words.\n",
    "    # filtered_tokens = [word for word in word_tokenize(text) if not word in stop_words]\n",
    "    # text = (\"\").join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cffda279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on no word root strip:\n",
      "F1 Score: 0.7850\n",
      "Accuracy: 0.7911\n",
      "\n",
      "Test on Stemmatize:\n",
      "F1 Score: 0.7834\n",
      "Accuracy: 0.7898\n",
      "\n",
      "Test on Lemmatize:\n",
      "F1 Score: 0.7843\n",
      "Accuracy: 0.7904\n"
     ]
    }
   ],
   "source": [
    "# preprocess data-> split normally\n",
    "# set random seed\n",
    "# preprocess data-> split normally\n",
    "# set random seed\n",
    "\n",
    "np.random.seed(0)\n",
    "df_ = df.sample(frac=1).reset_index()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_full = []\n",
    "for i in df_.text:\n",
    "    processed_full.append(preprocess(i))\n",
    "df_.text = processed_full\n",
    "\n",
    "# numtest = int(len(df_)/3.5)\n",
    "# df_test = df_[:int(numtest/2)].reset_index(drop=True)\n",
    "# df_dev = df_[int(numtest/2):numtest].reset_index(drop=True)\n",
    "numtest = int(len(df_)/5)\n",
    "df_train = df_[numtest:].reset_index(drop=True)\n",
    "df_test = df_[:numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "# Naive Bayes example run, using non clustered data first.\n",
    "# I'll use TF-IDF in this to vectorize data.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_test = m_nb.predict(tt_data)\n",
    "print('Test on no word root strip:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))\n",
    "print()\n",
    "tweet_tokenizer = sent_tokenize\n",
    "\n",
    "np.random.seed(0)\n",
    "df_ = df.sample(frac=1).reset_index()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_full = []\n",
    "for i in df_.text:\n",
    "    processed_full.append(preprocess(i,method='s',tokenizer=tweet_tokenizer))\n",
    "df_.text = processed_full\n",
    "\n",
    "# numtest = int(len(df_)/3.5)\n",
    "# df_test = df_[:int(numtest/2)].reset_index(drop=True)\n",
    "# df_dev = df_[int(numtest/2):numtest].reset_index(drop=True)\n",
    "numtest = int(len(df_)/5)\n",
    "df_train = df_[numtest:].reset_index(drop=True)\n",
    "df_test = df_[:numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "# Naive Bayes example run, using non clustered data first.\n",
    "# I'll use TF-IDF in this to vectorize data.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_test = m_nb.predict(tt_data)\n",
    "print('Test on Stemmatize:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))\n",
    "\n",
    "# preprocess data-> split normally\n",
    "# set random seed\n",
    "\n",
    "tokenizer = sent_tokenize\n",
    "\n",
    "np.random.seed(0)\n",
    "df_ = df.sample(frac=1).reset_index()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_full = []\n",
    "for i in df_.text:\n",
    "    processed_full.append(preprocess(i,method='l',tokenizer=tokenizer))\n",
    "df_.text = processed_full\n",
    "\n",
    "# numtest = int(len(df_)/3.5)\n",
    "# df_test = df_[:int(numtest/2)].reset_index(drop=True)\n",
    "# df_dev = df_[int(numtest/2):numtest].reset_index(drop=True)\n",
    "numtest = int(len(df_)/5)\n",
    "df_train = df_[numtest:].reset_index(drop=True)\n",
    "df_test = df_[:numtest].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "# Naive Bayes example run, using non clustered data first.\n",
    "# I'll use TF-IDF in this to vectorize data.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_test = m_nb.predict(tt_data)\n",
    "print()\n",
    "print('Test on Lemmatize:')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(test_label, pred_test, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(test_label, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_boot(df, n_clusters=2):\n",
    "    # Must accept pre-processed data as DF\n",
    "    \n",
    "    \"\"\" Pre-Cluster data before splitting to enhance generalization\"\"\"\n",
    "    dat = df.text\n",
    "    tfidf = TfidfVectorizer()\n",
    "    t_data = tfidf.fit_transform(dat)\n",
    "    pca = \n",
    "    cluster = KMeans(n_clusters=n_clusters).fit(t_data) # really bad clustering for loo\n",
    "    df['assign'] = cluster.labels_\n",
    "    \n",
    "    if n_clusters == 2:\n",
    "        if len(df[df['assign']==1]) > len(df[df['assign']==0]):\n",
    "            s, l = 0, 1\n",
    "        else:\n",
    "            s, l = 1, 0\n",
    "        len_valid = int(len(df[df['assign']==s])/2)\n",
    "        df_test = df[df['assign']==s][:len_valid].reset_index(drop=True)\n",
    "        df_dev = df[df['assign']==s][len_valid:].reset_index(drop=True)\n",
    "        df_train = df[df['assign']==l].reset_index(drop=True)\n",
    "    else:\n",
    "        groups = [] # list of two tuples of clusters\n",
    "        centroids = cluster.cluster_centers_\n",
    "    \n",
    "    # Simple prelim:: Sparse matrix for Spectral Clustering\n",
    "    # if cluster is more than 2 then use the majority of clusters\n",
    "    # closest to each other as training set and the rest as dev/test\n",
    "    # notes: this method did not work, the spectral takes too long for\n",
    "    # this size of a sparse matrix.\n",
    "    #     elif typ == 'cv':\n",
    "    #         cv = CountVectorizer()\n",
    "    #         t_data = cv.fit_transform(dat)\n",
    "    #         cluster = SpectralClustering(n_clusters=2).fit(t_data)\n",
    "    return (df_train, df_dev, df_test)\n",
    "\n",
    "df_train,df_dev,df_test = cluster_boot(df_)\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_t = m_nb.predict(tt_data)\n",
    "print('Metrics by cluster-splitting')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(dev_label, pred, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(dev_label, pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING \n",
    "dat = df_.text\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(dat)\n",
    "# not doing LSA, performing SVD is only for sake of cluster.. \n",
    "# due to very sparse data it is difficult to cluster on KMeans, \n",
    "# so we SVD I.E sparse->sparse to reduce dimensions\n",
    "svd = TruncatedSVD()\n",
    "t_data = svd.fit_transform(t_data)\n",
    "cluster = KMeans(n_clusters=2).fit(t_data) # really bad clustering for loo\n",
    "\n",
    "df['assign'] = cluster.labels_\n",
    "if len(df[df['assign']==1]) > len(df[df['assign']==0]):\n",
    "    s, l = 0, 1\n",
    "else:\n",
    "    s, l = 1, 0\n",
    "len_valid = int(len(df[df['assign']==s])/2)\n",
    "df_test = df[df['assign']==s][:len_valid].reset_index(drop=True)\n",
    "df_dev = df[df['assign']==s][len_valid:].reset_index(drop=True)\n",
    "df_train = df[df['assign']==l].reset_index(drop=True)\n",
    "\n",
    "train_data, train_label = np.array(df_train.text), np.array(df_train.target)\n",
    "dev_data, dev_label = np.array(df_dev.text), np.array(df_dev.target)\n",
    "test_data, test_label = np.array(df_test.text), np.array(df_test.target)\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(train_data)\n",
    "dt_data = tfidf.transform(dev_data)\n",
    "tt_data = tfidf.transform(test_data)\n",
    "m_nb = MultinomialNB(alpha=0.9).fit(t_data, train_label) # best alpha from project 3\n",
    "pred = m_nb.predict(dt_data)\n",
    "pred_t = m_nb.predict(tt_data)\n",
    "print('Metrics by cluster-splitting')\n",
    "print('F1 Score: {:.4f}'.format(metrics.f1_score(dev_label, pred, average='weighted')))\n",
    "print('Accuracy: {:.4f}'.format(metrics.accuracy_score(dev_label, pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.text\n",
    "tfidf = TfidfVectorizer()\n",
    "t_data = tfidf.fit_transform(dat)\n",
    "# not doing LSA, performing SVD is only for sake of cluster.. \n",
    "# due to very sparse data it is difficult to cluster on KMeans, \n",
    "# so we SVD I.E sparse->sparse to reduce dimensions\n",
    "svd = TruncatedSVD()\n",
    "t_data = svd.fit_transform(t_data)\n",
    "cluster = KMeans(n_clusters=6).fit(t_data) # really bad clustering for loo\n",
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cff311",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.spy(t_data,markersize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e04b8",
   "metadata": {},
   "source": [
    "# reference\n",
    "https://necromuralist.github.io/Neurotic-Networking/posts/nlp/01-twitter-preprocessing-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0da0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berkeley",
   "language": "python",
   "name": "berkeley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
