{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disaster tweets:\n",
      " 0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "\n",
      "Shape of train data: (7613, 5)\n",
      "\n",
      "Missing data in each column:\n",
      " id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Shape of test data: (3263, 4)\n",
      "\n",
      "Missing data in each column:\n",
      " id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read in data, very basic eda\n",
    "dftrain = pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\train.csv')\n",
    "train_data, train_labels = dftrain.text, dftrain.target\n",
    "print(\"Number of disaster tweets:\\n\", train_labels.value_counts())\n",
    "print(\"\\nShape of train data:\", dftrain.shape)\n",
    "print(\"\\nMissing data in each column:\\n\", dftrain.isnull().sum())\n",
    "#split train set into disaster and non disaster sets\n",
    "train_0 = dftrain.loc[dftrain.target == 0]\n",
    "train_1 = dftrain.loc[dftrain.target == 1]\n",
    "\n",
    "\n",
    "dftest = pd.read_csv(r'C:\\Users\\lwu31\\OneDrive - JNJ\\Documents\\test.csv')\n",
    "print(\"\\n\\nShape of test data:\", dftest.shape)\n",
    "print(\"\\nMissing data in each column:\\n\", dftest.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text prepocessing function.. not sure if/why the stopwords are not working\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import string\n",
    "def preprocess(text):\n",
    "    #convert all words to lowercase\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove digits\n",
    "    tokens_nonumbers = [token for token in tokens if not token.isdigit()]\n",
    "    #remove punctuation\n",
    "    tokens_nopunct = [token for token in tokens if not token in string.punctuation]\n",
    "    #remove \"stop words\"\n",
    "    stwords = stopwords.words('english')\n",
    "    stwords1 = ['http', 'https']\n",
    "    stwords = stwords + stwords1\n",
    "    tokens_nostop = [token for token in tokens if token not in stwords]\n",
    "    #apply stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    new_text = ' '.join(tokens_stem)\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find most common words for all train data\n",
    "vectorizer = CountVectorizer(preprocessor = preprocess, lowercase = True)\n",
    "#vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(dftrain.text)\n",
    "X = vectors.toarray()\n",
    "pos_df = pd.DataFrame(X, columns = vectorizer.get_feature_names())\n",
    "pos_dict = {word: pos_df[word].sum() for word in vectorizer.get_feature_names()}\n",
    "pos_df = pd.DataFrame(pos_dict, index = ['count'])\n",
    "pos_df = pos_df.T\n",
    "#pos_df.sort_values(by = 'count', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>4310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>3275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "co     4740\n",
       "http   4310\n",
       "the    3275\n",
       "in     1986\n",
       "to     1949\n",
       "of     1831\n",
       "and    1427\n",
       "is      969\n",
       "you     902\n",
       "for     894\n",
       "on      862\n",
       "it      860\n",
       "my      677\n",
       "that    628\n",
       "with    572"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for all train data\n",
    "pos_df.sort_values(by = 'count', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http co</th>\n",
       "      <td>2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in the</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https co</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of the</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>รป_ http</th>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicid bomber</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on the</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom bomb</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mass murder</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train derail</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more than</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by the</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have been</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at the</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern california</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "http co               2383\n",
       "in the                 145\n",
       "https co               133\n",
       "of the                 119\n",
       "รป_ http                109\n",
       "suicid bomber           60\n",
       "on the                  52\n",
       "atom bomb               50\n",
       "mass murder             47\n",
       "train derail            43\n",
       "more than               43\n",
       "by the                  42\n",
       "have been               42\n",
       "at the                  42\n",
       "northern california     41"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find most common words for only target = 1 train data\n",
    "#ok not super helpful with the stop words\n",
    "vectorizer = CountVectorizer(preprocessor = preprocess, lowercase = True,  ngram_range = (2,2))\n",
    "vectors_1 = vectorizer.fit_transform(train_1.text)\n",
    "X = vectors_1.toarray()\n",
    "pos_df = pd.DataFrame(X, columns = vectorizer.get_feature_names())\n",
    "pos_dict = {word: pos_df[word].sum() for word in vectorizer.get_feature_names()}\n",
    "pos_df = pd.DataFrame(pos_dict, index = ['count'])\n",
    "pos_df = pos_df.T\n",
    "pos_df.sort_values(by = 'count', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           vocab     coefs\n",
      "7878   hiroshima  2.497942\n",
      "17866    wildfir  2.399609\n",
      "5571   earthquak  2.127435\n",
      "15539      storm  1.982669\n",
      "16817    typhoon  1.938116\n",
      "10559    massacr  1.825315\n",
      "16488    tornado  1.787400\n",
      "15311      spill  1.757445\n",
      "6525       flood  1.733247\n",
      "5398     drought  1.727356\n",
      "11243     murder  1.685807\n",
      "6021       evacu  1.650567\n",
      "7544   hailstorm  1.626117\n",
      "15657     suicid  1.567204\n",
      "4942      derail  1.537439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwu31\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#train data to find most predictive words\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(vectors, train_labels)\n",
    "df_weights = pd.DataFrame()\n",
    "df_weights['vocab'] = vectorizer.get_feature_names()\n",
    "df_weights['coefs'] = lgr.coef_[0]\n",
    "df_weights.sort_values(by = 'coefs', ascending = False, inplace = True)\n",
    "print(df_weights.head(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earthquake', 'quake', 'temblor', 'seism']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lwu31\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "word = 'earthquake'\n",
    "for synonym in wordnet.synsets(word):\n",
    "   for item in synonym.lemmas():\n",
    "      if word != synonym.name() and len(synonym.lemma_names()) > 1:\n",
    "        synonyms.append(item.name())\n",
    "\n",
    "print(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hiroshima': ['hiroshima', 'hiroshima70'], 'wildfir': ['_wildfire__', 'calwildfir', 'dfir', 'idfir', 'wifi', 'wild', 'wildfir', 'wildlif'], 'earthquak': ['earth', 'earthquak', 'earthquake', 'earthquakenew', 'euroquak', 'megaquak'], 'storm': ['abstorm', 'custom', 'duststorm', 'hailstorm', 'histor', 'pastor', 'rainstorm', 'restor', 'sandstorm', 'sector', 'snowstorm', 'sto', 'store', 'storen', 'storey', 'stori', 'storm', 'stormchas', 'stormcom', 'stormy', 'stream', 'tom', 'tor', 'windstorm', 'yycstorm'], 'typhoon': ['typhoon', 'typo'], 'massacr': ['dmassa5', 'mascara', 'mass', 'massacr', 'massacre', 'massag'], 'tornado': ['ronaldo', 'tonysando', 'tora', 'tornado', 'trad', 'turdnado'], 'spill': ['ill', 'philli', 'pill', 'pillow', 'pll', 'silli', 'skill', 'spell', 'spi', 'spill', 'spilt', 'splatl', 'spoil', 'still'], 'flood': ['blood', 'bloodi', 'bloody', 'elwood', 'fleetwood', 'flood', 'floor', 'floored4', 'florid', 'floyd', 'foo', 'food', 'loo', 'ukflood'], 'drought': ['bought', 'brought', 'cadrought', 'doubleghat', 'doug', 'dough', 'drought', 'droughtgov', 'drug', 'fought', 'mydrought', 'ought', 'rout', 'sought', 'thought', 'through', 'throughout', 'wrought'], 'murder': ['moder', 'murder', 'order', 'rider', 'under'], 'evacu': ['devalu', 'eac4au', 'eau', 'ejacul', 'evac', 'evacid', 'evacu'], 'hailstorm': ['abstorm', 'hailstorm', 'hamilton', 'histor', 'histori', 'historic', 'rainstorm', 'storm', 'yahistor'], 'suicid': ['incid', 'juici', 'suffici', 'suicid', 'suicide', 'suspici'], 'derail': ['daili', 'deai', 'deal', 'debri', 'decal', 'derail', 'derbi', 'deriv', 'derma', 'detail', 'devil', 'drain', 'drill', 'email', 'eral', 'frail', 'ideal', 'rail', 'trail']}\n"
     ]
    }
   ],
   "source": [
    "#create dictionary for similar words\n",
    "#!pip install thefuzz\n",
    "from thefuzz import fuzz\n",
    "\n",
    "sim_list = [[sim for sim in vectorizer.get_feature_names() if fuzz.ratio(sim, word)>70] for word in df_weights['vocab'].head(15)]\n",
    "word_dict = {}\n",
    "for ind, word in enumerate(df_weights['vocab'].head(15)):\n",
    "    word_dict[word] = sim_list[ind]\n",
    "    \n",
    "print(word_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
